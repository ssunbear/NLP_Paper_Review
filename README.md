# NLP Paper Review

NLP와 관련된 논문을 읽고 정리한 레포입니다. 

## 구성 
|**Category**|**Paper**|**Date**|**Link**|
| ------------------------ | ------------| ----------- |----------- |
|`Seq2seq`|[**Sequence to Sequence Learning with Neural Networks**](https://github.com/ssunbear/NLP_Paper_Review/tree/main/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks)|`2024.08.30`|[링크](https://arxiv.org/abs/1409.3215)|
|`Transformer`|[**Attention is All You Need**](https://github.com/ssunbear/NLP_Paper_Review/tree/main/Attention%20is%20All%20You%20Need)|`2024.09.13`|[링크](https://arxiv.org/abs/1706.03762)|
|`PLM-Encoder` `ELMo`|[**Deep contextualized word representations**](https://github.com/ssunbear/NLP_Paper_Review/tree/main/Deep%20contextualized%20word%20representations)|`2024.09.13`|[링크](https://arxiv.org/abs/1802.05365)|
|`PLM-Encoder` `BERT`|[**BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**](https://github.com/ssunbear/NLP_Paper_Review/tree/main/BERT:%20Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding)|`2024.09.27`|[링크](https://arxiv.org/abs/1409.3215)|
|`PLM-Decoder` `GPT-2`|[**Language Models are Unsupervised Multitask Learners**](https://github.com/ssunbear/NLP_Paper_Review/tree/main/Language%20Models%20are%20Unsupervised%20Multitask%20Learners)|`2024.09.27`|[링크](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)|
|`PLM-seq2seq` `T5`|[**Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer**](https://github.com/ssunbear/NLP_Paper_Review/tree/main/Exploring%20the%20Limits%20of%20Transfer%20Learning%20with%20a%20Unified%20Text-to-Text%20Transformer)|`2024.10.17`|[링크](https://arxiv.org/abs/1910.10683)|
|`PLM-Decoder` `InstructGPT`|[**Training language models to follow instructions with human feedback**](https://github.com/ssunbear/NLP_Paper_Review/tree/main/Training%20language%20models%20to%20follow%20instructions%20with%20human%20feedback)|`2024.10.17`|[링크](https://arxiv.org/abs/2203.02155)|
|`PLM-Decoder` `LLaMA`|[**LLaMA: Open and Efficient Foundation Language Models**](https://github.com/ssunbear/NLP_Paper_Review/tree/main/LLaMA%3A%20Open%20and%20Efficient%20Foundation%20Language%20Models)|`2024.10.24`|[링크](https://arxiv.org/abs/2302.13971)|
|`Alignment Tuning` `DPO`|[**Direct Preference Optimization: Your Language Model is Secretly a Reward Model**](https://github.com/ssunbear/NLP_Paper_Review/tree/main/Direct%20Preference%20Optimization%3A%20Your%20Language%20Model%20is%20Secretly%20a%20Reward%20Model)|`2024.10.24`|[링크](https://arxiv.org/abs/2305.18290)|
|`Instruction Tuning` `LIMA`|[**LIMA: Less Is More for Alignment**]|`2024.10.31`|[링크](https://arxiv.org/abs/2305.11206)|
|`PEFT` `LoRA`|[**LoRA: Low-Rank Adaptation of Large Language Models**]|`2024.10.31`|[링크](https://arxiv.org/abs/2302.13971)|
|`Scaling Law`|[**Scaling Laws for Neural Language Models**]|`2024.11.07`|[링크](https://arxiv.org/abs/2001.08361)|
|`Scaling Law` `Chinchilla`|[**Training Compute-Optimal Large Language Models**]|`2024.11.07`|[링크](https://arxiv.org/abs/2203.15556)|
